{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9yj6iWdGzPzNldQsnsfkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasinadhsarma/Notebooks/blob/main/flightfare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# Task\n",
        "write it as torch and devlopment and integrate and devlop it faster\n",
        "\n",
        "Here is all the data you need:\n",
        "\"Data_Train.xlsx\"\n",
        "\"Sample_submission.xlsx\"\n",
        "\"Test_set.xlsx\""
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "DDxCgith6MZI"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the provided Excel files into pandas DataFrames.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "cmfKxIqE6Mo1"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Load the three Excel files into pandas DataFrames and inspect their initial contents and data types.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wqg3ZGbY6Nld"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df_train = pd.read_excel('Data_Train.xlsx')\n",
        "    df_submission = pd.read_excel('Sample_submission.xlsx')\n",
        "    df_test = pd.read_excel('Test_set.xlsx')\n",
        "\n",
        "    display(df_train.head())\n",
        "    display(df_submission.head())\n",
        "    display(df_test.head())\n",
        "\n",
        "    print(\"Data_Train info:\")\n",
        "    df_train.info()\n",
        "    print(\"\\nSample_submission info:\")\n",
        "    df_submission.info()\n",
        "    print(\"\\nTest_set info:\")\n",
        "    df_test.info()\n",
        "\n",
        "    print(\"\\nData_Train shape:\", df_train.shape)\n",
        "    print(\"Sample_submission shape:\", df_submission.shape)\n",
        "    print(\"Test_set shape:\", df_test.shape)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"One or more of the specified Excel files were not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TKpH4C3Z6N1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded DataFrames (`df_train`, `df_test`, `df_submission`) to understand their characteristics and identify potential issues.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "GCDZmSih6SrV"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Analyze data types, missing values, descriptive statistics, and visualize distributions for the training and test datasets. Also, check the submission format and calculate correlations.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "qwD19pfC6Thj"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Types and Missing Values\n",
        "print(\"Data Types and Missing Values:\")\n",
        "print(\"df_train:\\n\", df_train.isnull().sum())\n",
        "print(\"\\ndf_test:\\n\", df_test.isnull().sum())\n",
        "print(\"\\ndf_submission:\\n\", df_submission.isnull().sum())\n",
        "print(\"\\ndf_train data types:\\n\", df_train.dtypes)\n",
        "print(\"\\ndf_test data types:\\n\", df_test.dtypes)\n",
        "print(\"\\ndf_submission data types:\\n\", df_submission.dtypes)\n",
        "\n",
        "# Descriptive Statistics\n",
        "print(\"\\nDescriptive Statistics (df_train):\")\n",
        "print(df_train.describe(include='all'))\n",
        "\n",
        "# Distributions (Numerical Features)\n",
        "numerical_features = df_train.select_dtypes(include=['number'])\n",
        "for col in numerical_features.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(df_train[col], kde=True)\n",
        "    plt.title(f\"Distribution of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "# Distributions (Categorical Features)\n",
        "categorical_features = df_train.select_dtypes(exclude=['number'])\n",
        "for col in categorical_features.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    df_train[col].value_counts().plot(kind='bar')\n",
        "    plt.title(f\"Frequency of {col}\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "# Relationships\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df_train.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "for col in numerical_features.columns:\n",
        "    if col != 'Price':\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.scatterplot(x=col, y='Price', data=df_train)\n",
        "        plt.title(f\"{col} vs. Price\")\n",
        "        plt.show()\n",
        "\n",
        "for col in categorical_features.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x=col, y='Price', data=df_train)\n",
        "    plt.title(f\"Price vs {col}\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "# Data Shapes\n",
        "print(f\"\\ndf_train shape: {df_train.shape}\")\n",
        "print(f\"df_test shape: {df_test.shape}\")\n",
        "print(f\"df_submission shape: {df_submission.shape}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wslNnDFq6TxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed due to a `ValueError` when trying to calculate the correlation matrix because some columns are of object type.  I need to select only numerical columns for the correlation matrix and handle the errors gracefully when plotting.  I'll also improve the plotting to avoid overlapping plots and labels.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "X1nknkiE6ZZT"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ... (previous code for data types, missing values, descriptive stats)\n",
        "\n",
        "# Relationships (Corrected Correlation Matrix)\n",
        "numerical_features = df_train.select_dtypes(include=['number'])\n",
        "plt.figure(figsize=(10, 8))\n",
        "try:\n",
        "    sns.heatmap(numerical_features.corr(), annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix (Numerical Features)')\n",
        "except ValueError as e:\n",
        "    print(f\"Error creating correlation matrix: {e}\")\n",
        "plt.show()\n",
        "\n",
        "# Relationships (Scatter plots and box plots with error handling)\n",
        "for col in numerical_features.columns:\n",
        "    if col != 'Price':\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        try:\n",
        "            sns.scatterplot(x=col, y='Price', data=df_train)\n",
        "            plt.title(f\"{col} vs. Price\")\n",
        "        except Exception as e:\n",
        "          print(f\"Error creating scatter plot for {col}: {e}\")\n",
        "        plt.show()\n",
        "\n",
        "# ... (rest of the code for categorical features and data shapes)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SBjEsB3M6ZpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Prepare the data for model training by handling missing values, converting categorical features to numerical representations, and scaling numerical features. Apply consistent preprocessing to both training and testing datasets.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLZ-ljL6eOg"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Handle missing values in 'Route' and 'Total_Stops' columns of df_train, then engineer date and time features for both df_train and df_test.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ObuEOHra6fEw"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Handle missing values in df_train\n",
        "df_train['Route'].fillna(df_train['Route'].mode()[0], inplace=True)\n",
        "df_train['Total_Stops'].fillna(df_train['Total_Stops'].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "def preprocess_date_time(df):\n",
        "    df['Date_of_Journey'] = pd.to_datetime(df['Date_of_Journey'], format='%d/%m/%Y')\n",
        "    df['Day_of_Week'] = df['Date_of_Journey'].dt.dayofweek\n",
        "    df['Month'] = df['Date_of_Journey'].dt.month\n",
        "    df['Day'] = df['Date_of_Journey'].dt.day\n",
        "\n",
        "    df['Dep_Time'] = pd.to_datetime(df['Dep_Time'], format='%H:%M')\n",
        "    df['Dep_Hour'] = df['Dep_Time'].dt.hour\n",
        "\n",
        "    df['Arrival_Time'] = df['Arrival_Time'].str.replace('(\\d{1,2}:\\d{2})\\s[A-Za-z]*', '\\\\1')\n",
        "    df['Arrival_Time'] = pd.to_datetime(df['Arrival_Time'], format='%H:%M', errors='coerce')\n",
        "    df['Arrival_Hour'] = df['Arrival_Time'].dt.hour\n",
        "    df['Arrival_Hour'] = df['Arrival_Hour'].fillna(df['Arrival_Hour'].mode()[0])\n",
        "    return df\n",
        "\n",
        "df_train = preprocess_date_time(df_train)\n",
        "df_test = preprocess_date_time(df_test)\n",
        "\n",
        "display(df_train.head())\n",
        "display(df_test.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8A2R2ngG6fUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Encode categorical features and scale numerical features.  Handle unseen categories in the test set during encoding.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "7WrC6ROj6hHm"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Categorical features to encode\n",
        "categorical_cols = ['Airline', 'Source', 'Destination', 'Total_Stops', 'Additional_Info', 'Day_of_Week', 'Month']\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoders = {}\n",
        "\n",
        "# Fit and transform categorical features in df_train\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Transform categorical features in df_test, handling unseen labels\n",
        "for col in categorical_cols:\n",
        "    le = label_encoders[col]\n",
        "    df_test[col] = le.transform(df_test[col])\n",
        "\n",
        "\n",
        "# Numerical features to scale\n",
        "numerical_cols = ['Dep_Hour', 'Arrival_Hour']\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform numerical features in df_train\n",
        "df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n",
        "\n",
        "# Transform numerical features in df_test\n",
        "df_test[numerical_cols] = scaler.transform(df_test[numerical_cols])\n",
        "\n",
        "display(df_train.head())\n",
        "display(df_test.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7tjSmCn36hXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Engineer new features from existing ones in `df_train` and `df_test` to potentially improve model performance.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "qbht4HLS6mFz"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Convert 'Duration' to minutes and create interaction features.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uSNipFKE6m8G"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def convert_duration_to_minutes(duration):\n",
        "    if pd.isna(duration):\n",
        "        return np.nan\n",
        "    hours = 0\n",
        "    minutes = 0\n",
        "    duration_parts = duration.split(' ')\n",
        "    for part in duration_parts:\n",
        "        if 'h' in part:\n",
        "            hours = int(part[:-1])\n",
        "        elif 'm' in part:\n",
        "            minutes = int(part[:-1])\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "# Apply duration conversion\n",
        "df_train['Duration_minutes'] = df_train['Duration'].apply(convert_duration_to_minutes)\n",
        "df_test['Duration_minutes'] = df_test['Duration'].apply(convert_duration_to_minutes)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df_train['Duration_minutes'].fillna(df_train['Duration_minutes'].mean(), inplace=True)\n",
        "df_test['Duration_minutes'].fillna(df_test['Duration_minutes'].mean(), inplace=True)\n",
        "\n",
        "# Create interaction features\n",
        "df_train['Stops_Duration_Interaction'] = df_train['Total_Stops'] * df_train['Duration_minutes']\n",
        "df_test['Stops_Duration_Interaction'] = df_test['Total_Stops'] * df_test['Duration_minutes']\n",
        "\n",
        "df_train['Source_Dest_Interaction'] = df_train['Source'] * df_train['Destination']\n",
        "df_test['Source_Dest_Interaction'] = df_test['Source'] * df_test['Destination']\n",
        "\n",
        "display(df_train.head())\n",
        "display(df_test.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Nv66HSA16nLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Fix the chained assignment warnings and complete the remaining feature engineering steps.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "1PSGIwaK6o-v"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Fix chained assignment warnings\n",
        "df_train['Duration_minutes'] = df_train['Duration_minutes'].fillna(df_train['Duration_minutes'].mean())\n",
        "df_test['Duration_minutes'] = df_test['Duration_minutes'].fillna(df_test['Duration_minutes'].mean())\n",
        "\n",
        "# Polynomial features\n",
        "numerical_cols = ['Dep_Hour', 'Arrival_Hour', 'Duration_minutes']\n",
        "for col in numerical_cols:\n",
        "    df_train[col + '_squared'] = df_train[col] ** 2\n",
        "    df_test[col + '_squared'] = df_test[col] ** 2\n",
        "\n",
        "# Route Feature Engineering (simplified approach)\n",
        "df_train['Route_Stops'] = df_train['Route'].str.count('→')\n",
        "df_test['Route_Stops'] = df_test['Route'].str.count('→')\n",
        "\n",
        "# Fill NaN values in 'Route_Stops' with 0\n",
        "df_train['Route_Stops'] = df_train['Route_Stops'].fillna(0)\n",
        "df_test['Route_Stops'] = df_test['Route_Stops'].fillna(0)\n",
        "\n",
        "display(df_train.head())\n",
        "display(df_test.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EVnxEcKl6pOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data splitting\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed training data (`df_train`) into training and validation sets.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "KDnbahfk6tU_"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df_train.drop('Price', axis=1)\n",
        "y = df_train['Price']\n",
        "\n",
        "# Split data into training and validation sets without stratification\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lCi7tFIr6xDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a PyTorch model on the training data (X_train, y_train) and evaluate its performance on the validation data (X_val, y_val).\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "JI50ErBO61oX"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to convert the data to PyTorch tensors failed due to object dtypes.  I'll try a different approach by explicitly converting all columns to numeric types, handling potential errors gracefully.  This will ensure compatibility with PyTorch tensors.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "zGVTnv3n69gV"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert a column to numeric, handling errors\n",
        "def convert_to_numeric(df, column):\n",
        "    try:\n",
        "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting column '{column}': {e}\")\n",
        "        return df\n",
        "\n",
        "# Iterate through all columns in X_train and X_val and convert to numeric\n",
        "for col in X_train.columns:\n",
        "    X_train = convert_to_numeric(X_train, col)\n",
        "    if col in X_val.columns:\n",
        "        X_val = convert_to_numeric(X_val, col)\n",
        "\n",
        "# Fill NaN values with 0\n",
        "X_train = X_train.fillna(0)\n",
        "X_val = X_val.fillna(0)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rCZeQJQY69wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully converted the data to numeric types and created PyTorch tensors. Now I need to proceed with the model training and evaluation as outlined in the instructions.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "iRriwmI66_aq"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Define the model architecture (same as before)\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 2. Define the loss function and optimizer (same as before)\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = PricePredictionModel(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Training loop (same as before)\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    train_loss = criterion(outputs, y_train_tensor)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# 5. Save the model (same as before)\n",
        "torch.save(model.state_dict(), f'price_prediction_model_epoch_{num_epochs}_val_loss_{val_loss.item():.4f}.pth')\n",
        "\n",
        "# 6. Visualize the training and validation loss (same as before)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "C7EMM9fd6_tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a PyTorch model to predict flight prices.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt9iZRs07F_X"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to train the model failed due to data type issues.  Let's try a different approach to convert the data into PyTorch tensors.  I will convert all object columns to numeric using `pd.to_numeric(errors='coerce')`, replace infinite values with NaN, and then fill NaNs with 0.  After that, I will create and train a model.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "QiOEHFuV7HdQ"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert object columns to numeric, handling errors\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == 'object':\n",
        "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
        "    if col in X_val.columns and X_val[col].dtype == 'object':\n",
        "        X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_val.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Fill NaN values with 0\n",
        "X_train.fillna(0, inplace=True)\n",
        "X_val.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# 1. Define the model architecture\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 2. Define the loss function and optimizer\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = PricePredictionModel(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    train_loss = criterion(outputs, y_train_tensor)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# 5. Save the model\n",
        "torch.save(model.state_dict(), f'price_prediction_model_epoch_{num_epochs}_val_loss_{val_loss.item():.4f}.pth')\n",
        "\n",
        "# 6. Visualize the training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "om6yaOyV7H2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a PyTorch model to predict flight prices.  Address previous data preprocessing issues.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "yIYNO8Xf7M3X"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to train the model failed due to data preprocessing issues.  This time, I will apply one-hot encoding to the categorical features, standardize the numerical features, and handle outliers more robustly. Then, I will train the model again and evaluate its performance.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "SZrPb0En7O2m"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Airline', 'Source', 'Destination', 'Total_Stops', 'Additional_Info', 'Day_of_Week', 'Month', 'Route']\n",
        "numerical_cols = ['Dep_Hour', 'Arrival_Hour', 'Duration_minutes', 'Stops_Duration_Interaction', 'Source_Dest_Interaction', 'Dep_Hour_squared', 'Arrival_Hour_squared', 'Duration_minutes_squared', 'Route_Stops', 'Day']\n",
        "\n",
        "# Preprocessing for training data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other columns if any\n",
        ")\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val_processed.toarray() if hasattr(X_val_processed, 'toarray') else X_val_processed, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = PricePredictionModel(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    train_loss = criterion(outputs, y_train_tensor)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'price_prediction_model.pth')\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5J2Dg3iL7PLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a PyTorch model to predict flight prices.  Address previous data preprocessing issues and incorporate a more robust training strategy.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "lQBFhKVD7Utt"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous attempts at model training failed due to preprocessing issues.  This code will refine the preprocessing of the data, trying target encoding for high cardinality categorical features, handling outliers in numerical features using RobustScaler, and ensuring no `inf` or `nan` values are present in the tensors.  Then, it will train a more robust model with regularization and early stopping.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "aC5kZQu67XKK"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to a missing package `category_encoders`.  I will install the package and then re-run the code.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "CZjalPsZ7Zbz"
      }
    },
    {
      "source": [
        "!pip install category_encoders"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IBcQpfCQ7Zrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The `category_encoders` package is now installed. I will execute the previous code block again, which should now run successfully.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp9nyQld7d-y"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from category_encoders import TargetEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Airline', 'Source', 'Destination', 'Total_Stops', 'Additional_Info', 'Day_of_Week', 'Month', 'Route']\n",
        "numerical_cols = ['Dep_Hour', 'Arrival_Hour', 'Duration_minutes', 'Stops_Duration_Interaction', 'Source_Dest_Interaction', 'Dep_Hour_squared', 'Arrival_Hour_squared', 'Duration_minutes_squared', 'Route_Stops', 'Day']\n",
        "\n",
        "# Preprocessing for training data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', TargetEncoder())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val_processed, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Check for and handle inf or nan values\n",
        "def check_and_handle_inf_nan(tensor):\n",
        "    tensor[torch.isinf(tensor)] = 0\n",
        "    tensor[torch.isnan(tensor)] = 0\n",
        "    return tensor\n",
        "\n",
        "X_train_tensor = check_and_handle_inf_nan(X_train_tensor)\n",
        "y_train_tensor = check_and_handle_inf_nan(y_train_tensor)\n",
        "X_val_tensor = check_and_handle_inf_nan(X_val_tensor)\n",
        "y_val_tensor = check_and_handle_inf_nan(y_val_tensor)\n",
        "\n",
        "\n",
        "# Define the model with dropout and ELU activation\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.elu1 = nn.ELU()\n",
        "        self.dropout1 = nn.Dropout(0.3)  # Add dropout\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.elu2 = nn.ELU()\n",
        "        self.dropout2 = nn.Dropout(0.3) # Add dropout\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.elu3 = nn.ELU()\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.elu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.elu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.elu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = PricePredictionModel(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "num_epochs = 100 # Increased number of epochs\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    train_loss = criterion(outputs, y_train_tensor)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve == patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'price_prediction_model_robust.pth')\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "e1-Oojeo7eUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model optimization\n",
        "\n",
        "### Subtask:\n",
        "Optimize the PyTorch model's hyperparameters to improve its performance.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "GdStWzw37nRP"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Fix the `TypeError` by modifying the `PricePredictionModel` class to accept the `hidden_size` and `dropout_rate` arguments in the constructor and use them in the model definition.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "vc4Rz2WS7raN"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Define the model architecture with adjustable parameters\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_rate):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.elu1 = nn.ELU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.elu2 = nn.ELU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
        "        self.elu3 = nn.ELU()\n",
        "        self.fc4 = nn.Linear(hidden_size // 4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.elu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.elu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.elu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EPWqYVG97rve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code block had a `TypeError` because the `PricePredictionModel` class definition was not included in the code block. Now that the class is defined correctly, execute the hyperparameter optimization code.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "8yuud9GD7unM"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-4, 5e-4, 1e-3],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'hidden_size': [64, 128, 256],\n",
        "    'dropout': [0.2, 0.3, 0.4],\n",
        "    'optimizer': ['Adam', 'RMSprop']\n",
        "}\n",
        "\n",
        "# Number of folds for cross-validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Iterate through hyperparameter combinations\n",
        "for lr in param_grid['learning_rate']:\n",
        "    for bs in param_grid['batch_size']:\n",
        "        for hs in param_grid['hidden_size']:\n",
        "            for do in param_grid['dropout']:\n",
        "                for opt in param_grid['optimizer']:\n",
        "                    fold_losses = []\n",
        "                    for fold, (train_index, val_index) in enumerate(kf.split(X_train_tensor)):\n",
        "                        # Split data for the current fold\n",
        "                        X_train_fold, X_val_fold = X_train_tensor[train_index], X_train_tensor[val_index]\n",
        "                        y_train_fold, y_val_fold = y_train_tensor[train_index], y_train_tensor[val_index]\n",
        "\n",
        "                        # Create DataLoader for current fold\n",
        "                        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
        "                        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "                        # Initialize model\n",
        "                        model = PricePredictionModel(input_size=X_train_tensor.shape[1], hidden_size=hs, dropout_rate=do)\n",
        "\n",
        "                        # Initialize optimizer\n",
        "                        if opt == 'Adam':\n",
        "                            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                        else:  # RMSprop\n",
        "                            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "                        criterion = nn.MSELoss()\n",
        "\n",
        "                        # Training loop with early stopping\n",
        "                        best_val_loss = float('inf')\n",
        "                        patience = 5\n",
        "                        epochs_no_improve = 0\n",
        "                        for epoch in range(100):  # Max epochs\n",
        "                            for batch_X, batch_y in train_loader:\n",
        "                                optimizer.zero_grad()\n",
        "                                outputs = model(batch_X)\n",
        "                                loss = criterion(outputs, batch_y)\n",
        "                                loss.backward()\n",
        "                                optimizer.step()\n",
        "\n",
        "                            with torch.no_grad():\n",
        "                                val_outputs = model(X_val_fold)\n",
        "                                val_loss = criterion(val_outputs, y_val_fold)\n",
        "\n",
        "                            if val_loss < best_val_loss:\n",
        "                                best_val_loss = val_loss\n",
        "                                epochs_no_improve = 0\n",
        "                            else:\n",
        "                                epochs_no_improve += 1\n",
        "                                if epochs_no_improve == patience:\n",
        "                                    break\n",
        "\n",
        "                        fold_losses.append(best_val_loss.item())\n",
        "\n",
        "                    avg_loss = np.mean(fold_losses)\n",
        "                    results.append({\n",
        "                        'learning_rate': lr,\n",
        "                        'batch_size': bs,\n",
        "                        'hidden_size': hs,\n",
        "                        'dropout': do,\n",
        "                        'optimizer': opt,\n",
        "                        'avg_val_loss': avg_loss\n",
        "                    })\n",
        "                    print(f\"Hyperparameters: {lr, bs, hs, do, opt}, Avg Val Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Find best hyperparameters\n",
        "best_params = min(results, key=lambda x: x['avg_val_loss'])\n",
        "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
        "\n",
        "# Retrain with best hyperparameters on the entire training set\n",
        "model = PricePredictionModel(input_size=X_train_tensor.shape[1], hidden_size=best_params['hidden_size'], dropout_rate=best_params['dropout'])\n",
        "if best_params['optimizer'] == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "else:\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=best_params['learning_rate'])\n",
        "criterion = nn.MSELoss()\n",
        "# Training loop (adjust epochs as needed)\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save the final model\n",
        "torch.save(model.state_dict(), 'final_price_prediction_model.pth')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KHmheGOI7u2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}